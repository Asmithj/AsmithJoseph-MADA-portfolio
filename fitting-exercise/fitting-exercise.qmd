---
title: "Model Fitting Exercise"
author: "Asmith Joseph"
date: "02/26/2025"
format: html
editor: 
  markdown: 
    wrap: sentence
---

# Setup

```{r}
#load needed packages. make sure they are installed.
library(here) #for data loading/saving
library(dplyr)
library(readr)
library(skimr)
library(ggplot2)
library(kknn)
```

**Load the data**

```{r}
# Load the packages
library(here)
library(readr)

# Define the file path using here()
file_path <- here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv")

# Load the dataset
Mavoglurant_A2121_nmpk <- read_csv(file_path)

# Display first few rows
head(Mavoglurant_A2121_nmpk)
```

# Data exploration

```{r}
# View dataset structure
str(Mavoglurant_A2121_nmpk)
```

```{r}
# Check column names
colnames(Mavoglurant_A2121_nmpk)
```

```{r}
# Summary statistics
summary(Mavoglurant_A2121_nmpk)
```

```{r}
# Count missing values per column
colSums(is.na(Mavoglurant_A2121_nmpk))

```

**Creating visual to demonstrate DV Over Time for Each Individual, Faceted by Dose**

```{r}
library(ggplot2)

# Creating figure of DV by time for each person by dose level
ggplot(Mavoglurant_A2121_nmpk, aes(x = TIME, y = DV, group = ID)) +
  geom_line(alpha = 0.7, color = "blue") +  # Adds individual lines
  geom_point(size = 1, alpha = 0.5) +  # Adds observed data points
  facet_wrap(~DOSE) +  # Facet by DOSE to create separate plots for each dose level
  labs(title = "Individual Response Over Time by Dose Level",
       x = "Time (hrs)",
       y = "Dependent Variable (DV)") +
  theme_minimal()

```

```{r}
ggplot(Mavoglurant_A2121_nmpk, aes(x = TIME, y = DV, group = ID, color = as.factor(ID))) +
  geom_line(alpha = 0.7) +
  geom_point(size = 1, alpha = 0.5) +
  facet_wrap(~DOSE) +
  labs(title = "Individual Response Over Time by Dose Level",
       x = "Time (hrs)",
       y = "Dependent Variable (DV)",
       color = "Individual ID") +
  theme_minimal()

```

**Filtering the dataset to keep only observations where OCC = 1**

```{r}

library(dplyr)

# Filter the dataset to keep only OCC = 1
Mavoglurant_A2121_nmpk_OCC1 <- Mavoglurant_A2121_nmpk %>%
  filter(OCC == 1)

# Display first few rows
head(Mavoglurant_A2121_nmpk_OCC1)
```

-   Doing a combination of the Baseline Observations with Summed Dependent Variable (DV) Values
-   I removed observations where TIME = 0 to create a filtered dataset.
-   Next, I computed the sum of DV for each individual (ID) using dplyr::summarize(), storing the result as a 120 x 2 data frame with columns ID and Y.
-   I then extract only the observations where TIME = 0, resulting in a 120 x 17 data frame. +Finally, I ed the two datasets using left_join() on ID, creating a final 120 x 18 data frame that combines the TIME = 0 data with the summed DV values (Y).

```{r}
library(dplyr)

# Remove rows where TIME == 0
data_no_time0 <- Mavoglurant_A2121_nmpk %>%
  filter(TIME != 0)



# Summarize DV sum per ID
sum_DV_per_ID <- data_no_time0 %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))

# Check result
dim(sum_DV_per_ID)  # Expected: 120 x 2
head(sum_DV_per_ID)



# Keep only TIME == 0
data_time0 <- Mavoglurant_A2121_nmpk %>%
  filter(TIME == 0)

# Check result
dim(data_time0)  # Expected: 120 x 17
head(data_time0)


# Join data_time0 (120x17) with sum_DV_per_ID (120x2) using ID
Mav.final_data <- left_join(data_time0, sum_DV_per_ID, by = "ID")

# Check final result
dim(Mav.final_data)  # Expected: 120 x 18
head(Mav.final_data)




```

**Data Transformation Converting Factors and Selecting Key Variables** - I transformed the dataset by converting RACE and SEX into factor variables to ensure proper categorical data handling.
Then, I selected only the relevant variables: Y (sum of DV for each individual), DOSE, AGE, SEX, RACE, WT, and HT, creating a refined dataset for further analysis.
This process helps streamline the data, ensuring that categorical variables are correctly classified while retaining only the essential information for modeling and interpretation.

```{r}
library(dplyr)

# Convert RACE and SEX to factors and keep only selected variables
Mav.final_data_selected <- Mav.final_data %>%
  mutate(
    RACE = as.factor(RACE),
    SEX = as.factor(SEX)
  ) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)

# Check structure of the new dataset
str(Mav.final_data_selected)

# View first few rows
head(Mav.final_data_selected)

```

```{r}
# Create a summary table of final_data_selected

library(gtsummary)

Mav.final_data_selected %>%
  tbl_summary()

```

# More Data exploration through figures & Tables

-   Exploratory Data Analysis Summary Tables and plots were generated to explore relationships between total drug exposure (Y) and key predictors.
-   Summary tables provided descriptive statistics, while scatterplots and boxplots visualized trends between Y, AGE, DOSE, and SEX, highlighting dose-response effects.
-   Histograms and density plots examined variable distributions for skewness and anomalies. Lastly, a correlation matrix and scatterplot pairs identified significant predictors of Y, offering key insights into the dataset's structure.

Tables

```{r}

library(gtsummary)
library(skimr)

# Summary table using gtsummary
Mav.final_data_selected %>%
  tbl_summary(
    statistic = list(all_continuous() ~ "{mean} ({sd})", 
                     all_categorical() ~ "{n} ({p}%)"),
    digits = all_continuous() ~ 2
  )

# More detailed summary using skimr
skim(Mav.final_data_selected)

```

Scatterplots & Boxplots

```{r}
library(ggplot2)

ggplot(Mav.final_data_selected, aes(x = AGE, y = Y, color = SEX)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Total Drug Exposure (Y) vs. Age", x = "Age", y = "Total Drug (Y)") +
  theme_minimal()

```

Density Plot of WT (Weight)

```{r}
ggplot(Mav.final_data_selected, aes(x = as.factor(DOSE), y = Y, fill = SEX)) +
  geom_boxplot() +
  labs(title = "Total Drug (Y) Distribution Across Dose Levels", x = "Dose Level", y = "Total Drug (Y)") +
  theme_minimal()

```

Total Drug

```{r}
ggplot(Mav.final_data_selected, aes(x = Y)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.6) +
  labs(title = "Distribution of Total Drug (Y)", x = "Total Drug (Y)", y = "Count") +
  theme_minimal()

```

Density Plot of WT (Weight)

```{r}
ggplot(Mav.final_data_selected, aes(x = WT, fill = SEX)) +
  geom_density(alpha = 0.5) +
  labs(title = "Weight Distribution by Sex", x = "Weight (kg)", y = "Density") +
  theme_minimal()

```

Relationships among continuous variables

```{r}

library(GGally)

# Pairwise scatterplots and correlations
ggpairs(Mav.final_data_selected, columns = c("Y", "DOSE", "AGE", "WT", "HT"))

```

### Exploratory Data Analysis (EDA) Note:

## **Summary Table:**

-   This table provides key descriptive statistics (N=198) for variables including Y, DOSE, AGE, SEX, RACE, WT, and HT, highlighting their median values and distributions. - Individual Response Over Time by Dose (3 Panels): Each panel displays the DV time course for a specific dose level (25, 37.5, 50), showing higher peaks for higher doses and notable inter-individual variability.
-   Colored Individual Response Over Time: This plot again shows DV time profiles by dose level, color-coding each individual to emphasize the variability in response within each dose group.
-   Scatterplot (Total Drug Y vs. Age by Sex): The scatterplot indicates a potential negative trend of total drug (Y) with increasing age, with sex differences visible in the distribution. - Boxplots (Total Drug Y by Dose Level and Sex): These boxplots illustrate how total drug exposure (Y) varies across different dose levels (25, 37.5, 50) and between sexes, showing higher medians at the highest dose.
-   Histogram (Distribution of Total Drug Y): The histogram reveals the overall distribution of total drug (Y), centered around 3000–5000, with a slight skew toward higher values.
-   Weight Distribution by Sex (Density Plot): The density plot compares weight distributions between sexes, suggesting one group has a generally higher weight range than the other.
-   Correlation Matrix (Pairs Plot): This matrix highlights moderate correlations among WT, HT, and Y, while DOSE and AGE show weaker relationships with Y.

## \# Model fitting

Model Fitting Summary In this section, I fit two linear regression models using the tidymodels framework to predict total drug exposure (Y).
First, I built a simple linear model (Y \~ DOSE) to assess the effect of dosage alone.
Next, I expanded to a multiple regression model (Y \~ DOSE + AGE + SEX + RACE + WT + HT) to evaluate additional predictors.
I then computed RMSE and R² for both models to compare their performance.
While the multiple model slightly improved prediction accuracy, both models had low R² values, suggesting that important factors influencing Y are missing, warranting further exploration of nonlinear models or alternative predictors.

```{r}
# Load necessary libraries
library(tidymodels)
library(dplyr)

```

Defining the Data for Modeling

```{r}
# Ensure categorical variables are factors
Mav.final_data_selected <- Mav.final_data_selected %>%
  mutate(
    SEX = as.factor(SEX),
    RACE = as.factor(RACE)
  )

# Split data into training (80%) and testing (20%)
set.seed(123)  # For reproducibility
data_split <- initial_split(Mav.final_data_selected, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

```

Fitting a Simple Linear Model (Y \~ DOSE)

```{r}
# Define the linear model
simple_model <- linear_reg() %>%
  set_engine("lm")

# Define the recipe (formula-based)
simple_recipe <- recipe(Y ~ DOSE, data = train_data)

# Bundle model and recipe into a workflow
simple_workflow <- workflow() %>%
  add_model(simple_model) %>%
  add_recipe(simple_recipe)

# Fit the model to the training data
simple_fit <- simple_workflow %>% fit(data = train_data)

# Print model summary
tidy(simple_fit$fit$fit)

```

## **Note/Interpretation:**

-   The simple linear regression model predicts total drug exposure (Y) using DOSE as the only predictor. +The intercept (β₀ = 4008.11) suggests a baseline drug exposure when DOSE = 0, while the slope (β₁ = 6.83) indicates that each unit increase in DOSE results in a small, statistically insignificant increase in Y (p = 0.40).
-   The p-value suggests that DOSE alone is not a significant predictor of Y, implying that other factors like AGE, SEX, WT, or HT may better explain variability in total drug exposure.
-   A multiple regression model incorporating these variables may improve predictive power.

**Fit a Multiple Linear Model (Y \~ All Predictors)**

```{r}
# Define the recipe including all predictors
full_recipe <- recipe(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = train_data)

# Bundle the full model into a workflow
full_workflow <- workflow() %>%
  add_model(simple_model) %>%
  add_recipe(full_recipe)

# Fit the full model to the training data
full_fit <- full_workflow %>% fit(data = train_data)

# Print model summary
tidy(full_fit$fit$fit)

```

## **Note/Interpretation:**

-   Multiple Linear Regression Model Summary This model predicts total drug exposure (Y) using multiple predictors: DOSE, AGE, SEX, RACE, WT, and HT.
-   The intercept (β₀ = 5692.48) represents the baseline drug exposure when all predictors are at zero. +The DOSE coefficient (β₁ = 10.05) suggests a positive effect, but it is not statistically significant (p = 0.21). Other predictors, including AGE, SEX, and RACE, also have high p-values, indicating weak individual associations with Y.
-   Weight (WT, p = 0.052) is the closest to significance, suggesting it may have some predictive influence.
-   The large standard errors for some predictors indicate high variability, potentially due to multicollinearity or insufficient data variation.
-   Overall, while adding predictors slightly adjusts the model, none appear to be strong independent predictors of Y, suggesting further feature selection or alternative modeling approaches may be beneficial.

**Compute RMSE & R² for Both Models**

```{r}
# Define function to compute performance metrics
compute_metrics <- function(model_fit, test_data) {
  predictions <- predict(model_fit, new_data = test_data) %>%
    bind_cols(test_data)  # Merge predictions with test data

  # Compute RMSE and R²
  metrics <- predictions %>%
    metrics(truth = Y, estimate = .pred) %>%
    filter(.metric %in% c("rmse", "rsq"))

  return(metrics)
}

# Compute metrics for both models
simple_metrics <- compute_metrics(simple_fit, test_data)
full_metrics <- compute_metrics(full_fit, test_data)

# Print results
print("Performance Metrics for Simple Model (Y ~ DOSE)")
print(simple_metrics)

print("Performance Metrics for Full Model (Y ~ All Predictors)")
print(full_metrics)

```

Model Performance Comparison (Simple vs. Multiple Regression) The multiple regression model slightly outperforms the simple model (Y \~ DOSE), with a lower RMSE (1213.57 vs. 1245.70) and a higher R² (3.19% vs. 0.67%), but both models show poor predictive power.
The high RMSE suggests large prediction errors, while the low R² values indicate that neither DOSE nor the additional predictors explain much of the variance in Y.
This suggests that important factors are missing, and alternative approaches, such as feature selection, nonlinear modeling, or interaction terms, may be needed for better predictions.

# Fitting a Logistic Regression Model for a Categorical Outcome (SEX)

In this section, I fit two logistic regression models where SEX (binary outcome: Male/Female) is the dependent variable.
While predicting SEX from DOSE and other predictors may not have a clear scientific basis.

I will: - a) Fit a simple logistic regression model predicting SEX using DOSE alone.
- b) Fit a multiple logistic regression model predicting SEX using all predictors (DOSE, AGE, Y, RACE, WT, HT).
c) Evaluate both models using accuracy and ROC-AUC (Receiver Operating Characteristic - Area Under the Curve).

**Data preparation for Logistic Regression**

```{r}
# Ensure SEX is a factor (binary categorical outcome)
Mav.final_data_selected <- Mav.final_data_selected %>%
  mutate(
    SEX = as.factor(SEX),  # Ensure SEX is a factor
    RACE = as.factor(RACE) # Keep RACE as a factor
  )

# Split data into training (80%) and testing (20%)
set.seed(123)  # For reproducibility
data_split <- initial_split(Mav.final_data_selected, prop = 0.8, strata = SEX)
train_data <- training(data_split)
test_data <- testing(data_split)

```

**Fit a Simple Logistic Model (SEX \~ DOSE)**

```{r}
# Define a logistic regression model
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Define the recipe (formula-based)
simple_recipe <- recipe(SEX ~ DOSE, data = train_data)

# Bundle model and recipe into a workflow
simple_workflow <- workflow() %>%
  add_model(logistic_model) %>%
  add_recipe(simple_recipe)

# Fit the model to the training data
simple_fit <- simple_workflow %>% fit(data = train_data)

# Print model summary
tidy(simple_fit$fit$fit)

```

## **Note/Interpretation:**

-   Logistic Regression Summary (SEX \~ DOSE) The intercept (-1.98, p = 0.011) suggests that the baseline probability of SEX (likely Male) at DOSE = 0 is statistically significant. However, the DOSE coefficient (0.0012, p = 0.95) indicates that DOSE has no meaningful effect on predicting SEX, as the p-value is very high.
-   This confirms that drug dosage is independent of gender, and additional predictors may be needed for better classification.

**Fit a Multiple Logistic Model (SEX \~ All Predictors)**

```{r}
# Define the recipe including all predictors
full_recipe <- recipe(SEX ~ DOSE + AGE + Y + RACE + WT + HT, data = train_data)

# Bundle the full model into a workflow
full_workflow <- workflow() %>%
  add_model(logistic_model) %>%
  add_recipe(full_recipe)

# Fit the full model to the training data
full_fit <- full_workflow %>% fit(data = train_data)

# Print model summary
tidy(full_fit$fit$fit)

```

## **Note/Interpretation**

-   Logistic Regression Summary (SEX \~ All Predictors) This model predicts SEX using multiple predictors (DOSE, AGE, Y, RACE, WT, HT).
-   The intercept (42.40, p \< 0.001) is statistically significant, but most predictors, including DOSE (p = 0.90), AGE (p = 0.12), and Y (p = 0.72), are not significant, indicating they have little effect on predicting SEX.
-   The only significant predictor is HT (p = 0.0018), suggesting that height may be somewhat informative for classifying SEX. However, overall, the model's weak predictor significance suggests poor classification power, and alternative approaches may be needed.

Compute Accuracy & ROC-AUC for Both Models

```{r}
compute_metrics <- function(model_fit, test_data) {
  # Generate class predictions
  class_preds <- predict(model_fit, new_data = test_data, type = "class") %>%
    bind_cols(test_data) %>%
    rename(.pred_class = .pred_class)  # Ensure column name is correct

  # Generate probability predictions
  prob_preds <- predict(model_fit, new_data = test_data, type = "prob") %>%
    bind_cols(test_data)

  # Compute ROC-AUC (Assuming "1" is the positive class)
  roc_auc_score <- roc_auc(prob_preds, truth = SEX, .pred_1)

  # Compute Accuracy
  accuracy_score <- accuracy(class_preds, truth = SEX, estimate = .pred_class)

  # Combine results
  metrics <- bind_rows(roc_auc_score, accuracy_score)

  return(metrics)
}

# Compute metrics for both models
simple_metrics <- compute_metrics(simple_fit, test_data)
full_metrics <- compute_metrics(full_fit, test_data)

# Print results
print("Performance Metrics for Simple Model (SEX ~ DOSE)")
print(simple_metrics)

print("Performance Metrics for Full Model (SEX ~ All Predictors)")
print(full_metrics)



```

## **Note/Interpretation**

-   Logistic Model Performance Comparison The simple logistic model (SEX \~ DOSE) performs poorly (ROC-AUC = 0.44), indicating that DOSE alone does not predict SEX. In contrast, the full model (SEX \~ All Predictors) shows high accuracy (95%) and strong discrimination (ROC-AUC = 0.97), suggesting that variables like HT, WT, or RACE contribute significantly to predicting SEX. However, the high accuracy may indicate overfitting, requiring further validation.

# K-Nearest Neighbors (KNN) Model Using Tidymodels

To further explore the dataset, we will fit a K-Nearest Neighbors (KNN) model to both: The continuous outcome (Y) and The categorical outcome (SEX) I will then compare the KNN model’s performance with the previous linear and logistic regression models.

**Preparing the Data to Ensure categorical variables are correctly formatted and split the data into training and testing sets**

```{r}
# Convert categorical variables to factors
Mav.final_data_selected <- Mav.final_data_selected %>%
  mutate(
    SEX = as.factor(SEX),
    RACE = as.factor(RACE)
  )

# Split data into training (80%) and testing (20%)
set.seed(123)
data_split <- initial_split(Mav.final_data_selected, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

```

# Fit a KNN Model for the Continuous Outcome (Y)

I will use KNN regression to predict Y (total drug exposure) using all predictors

```{r}
# Define KNN model
knn_reg_model <- nearest_neighbor(neighbors = 5) %>%
  set_engine("kknn") %>%
  set_mode("regression")

# Define recipe for regression (normalize only numeric predictors)
knn_reg_recipe <- recipe(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = train_data) %>%
  step_normalize(all_numeric_predictors()) # Normalize only numeric predictors

# Create workflow
knn_reg_workflow <- workflow() %>%
  add_model(knn_reg_model) %>%
  add_recipe(knn_reg_recipe)

# Fit the model
knn_reg_fit <- knn_reg_workflow %>% fit(data = train_data)
# Predict and compute RMSE & R-squared
knn_reg_metrics <- knn_reg_fit %>%
  predict(new_data = test_data) %>%
  bind_cols(test_data) %>%
  metrics(truth = Y, estimate = .pred) %>%
  filter(.metric %in% c("rmse", "rsq"))

print("Performance Metrics for KNN Regression Model (Y ~ All Predictors)")
print(knn_reg_metrics)

```

## **Note/Interpretation**

-   KNN Regression Model Performance (Y \~ All Predictors) The K-Nearest Neighbors (KNN) regression model shows a high RMSE (1393.16) and a very low R² (0.00027), indicating that the model performs poorly at predicting Y.
-   The high RMSE suggests large prediction errors, while the near-zero R² means the predictors explain almost none of the variance in Y.
-   This suggests that KNN may not be a suitable method for modeling this dataset, possibly due to the high-dimensional space or insufficient relevant patterns in the predictors.

# Fit a KNN Model for the Categorical Outcome (SEX)

I will use KNN classification to predict SEX using all predictors.

```{r}
# Define KNN model for classification
knn_class_model <- nearest_neighbor(neighbors = 5) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Define recipe for classification (convert factors and normalize only numeric variables)
knn_class_recipe <- recipe(SEX ~ DOSE + AGE + Y + RACE + WT + HT, data = train_data) %>%
  step_dummy(RACE) %>%  # Convert RACE to dummy variables
  step_normalize(all_numeric_predictors()) # Normalize only numeric predictors

# Create workflow
knn_class_workflow <- workflow() %>%
  add_model(knn_class_model) %>%
  add_recipe(knn_class_recipe)

# Fit the model
knn_class_fit <- knn_class_workflow %>% fit(data = train_data)

# Predict and compute accuracy & ROC-AUC
knn_class_metrics <- knn_class_fit %>%
  predict(new_data = test_data, type = "class") %>%
  bind_cols(test_data) %>%
  metrics(truth = SEX, estimate = .pred_class) %>%
  filter(.metric %in% c("accuracy"))

# Compute ROC-AUC
knn_class_roc_auc <- knn_class_fit %>%
  predict(new_data = test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_1)

print("Performance Metrics for KNN Classification Model (SEX ~ All Predictors)")
print(knn_class_metrics)
print(knn_class_roc_auc)

```

**Note/Interpretation**

-   The KNN classification model (SEX \~ All Predictors) performs exceptionally well, with a ROC-AUC of 0.98 and accuracy of 95%, indicating excellent discrimination and high classification accuracy.
-   This suggests that the model effectively predicts SEX based on variables like DOSE, AGE, Y, RACE, WT, and HT. However, the high accuracy may indicate overfitting, meaning the model might not generalize well to new data. Further cross-validation or tuning the number of neighbors (k) could help assess its robustness

## \# Conclusions of the Mavoglurant PK Analysis

This analysis examined the pharmacokinetics of Mavoglurant, focusing on the relationship between total drug exposure (Y) and key predictors.
DOSE was not a strong predictor of Y, as linear regression models showed low explanatory power.
Logistic regression revealed that HT was a significant factor in predicting SEX, while DOSE had little impact.
KNN performed well for classification (ROC-AUC = 0.98) but poorly for regression (R² ≈ 0), suggesting that Y may require alternative modeling approaches.
Future work should explore nonlinear models, feature engineering, and hyperparameter tuning to improve predictive performance.

------------------------------------------------------------------------

# Model Improvement (Part 1)

*Part 1: Setting a random seed* Since some steps in data analysis involve randomness (like splitting the dataset), I wanted to make sure that every time I run my code, I get the same results.
Setting a random seed keeps everything consistent, so I don’t have to worry about getting a different train/test split every time I run my script.

```{r}
# Set a seed value at the start
rngseed <- 1234

```

# Data Preparation:

*Removing the RACE Variable* - The RACE column had some weird values (7, 88), which didn’t seem reliable.
Instead of guessing what those values should be, I decided to drop the column completely to avoid any issues later in the analysis.
- I also selected only the important variables (Y, DOSE, AGE, SEX, WT, HT) since those are the ones I’ll be working with.
To make sure my dataset was set up correctly, I filtered it down to 120 observations, keeping only one row per unique ID.
This ensures I’m working with clean and properly formatted data before moving on.

```{r}
# Load required libraries
library(dplyr)
library(tidymodels)

# Set random seed for reproducibility
rngseed <- 1234
set.seed(rngseed)

# Select Necessary Columns & Remove RACE
Mav.final_data_selected <- Mav.final_data %>%
  select(ID, Y, DOSE, AGE, SEX, WT, HT) %>%  # Ensure we keep ID for uniqueness
  distinct(ID, .keep_all = TRUE) %>%  # Keep only one row per unique ID
  select(-ID)  # Remove ID column after filtering

# Ensure Exactly 120 Observations
Mav.final_data_selected <- Mav.final_data_selected %>%
  slice(1:120)  # Restrict to 120 rows (if needed)

# Verify the dataset size
print(dim(Mav.final_data_selected))  # Expected output: [1] 120 6
print(colnames(Mav.final_data_selected))  # Expected: "Y", "DOSE", "AGE", "SEX", "WT", "HT"


```

#Setting the Seed & Performing Train/Test Split\* Now that my dataset was clean and formatted correctly, I split it into training (75%) and testing (25%).
This is a standard approach when working with models: - Training set (90 x 6) → The model will learn patterns from this data.
- Testing set (30 x 6) → This will allow me to see how well my model performs on unseen data.
Since I had already set my random seed, I know that every time I run this split, I’ll get the same results, making my workflow more stable and reproducible.

```{r}
# Perform 75% Train / 25% Test Split
set.seed(rngseed)  # Reset seed before sampling
data_split <- initial_split(Mav.final_data_selected, prop = 0.75)

# Extract Training and Testing Sets
train_data <- training(data_split)
test_data <- testing(data_split)

# Verify Train/Test Set Sizes
print(dim(train_data))  
print(dim(test_data))   


```

#Model fitting

#Define & Fit the First Model (Y \~ DOSE) *linear regression model where Y (total drug exposure) is predicted only using DOSE* I'm fitting a simple linear regression model where Y (total drug exposure) is predicted using only DOSE.
This helps me see how much DOSE alone explains the variability in Y.

```{r}
# Load necessary libraries
library(tidymodels)

# Set random seed for reproducibility
set.seed(1234)



# Define a simple linear model
simple_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Create a recipe using only DOSE as the predictor
simple_recipe <- recipe(Y ~ DOSE, data = train_data)

# Create a workflow
simple_workflow <- workflow() %>%
  add_model(simple_model) %>%
  add_recipe(simple_recipe)

# Fit the model on the training dataset
simple_fit <- simple_workflow %>% fit(data = train_data)

# Print model summary
print(tidy(simple_fit$fit$fit))

```

*Interpretation:* - The intercept (3686.07, p \< 0.001) is highly significant, meaning that when DOSE = 0, the expected drug exposure (Y) is around 3686 units.
However, the coefficient for DOSE (10.93, p = 0.36) is not statistically significant, suggesting that dose alone does not strongly predict Y.
This means other factors might play a bigger role in determining drug exposure, so I need to check the full model to see if additional predictors improve accuracy.

#Define & Fit the Second Model (Y \~ All Predictors) *multiple linear regression model, which uses all predictors to predict Y* Now, I’m fitting a multiple linear regression model that uses all available predictors (DOSE, AGE, SEX, WT, HT) to predict Y.
This helps me see if including more variables improves model performance.

```{r}
# Define a multiple linear regression model
full_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Create a recipe using all predictors
full_recipe <- recipe(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)

# Create a workflow
full_workflow <- workflow() %>%
  add_model(full_model) %>%
  add_recipe(full_recipe)

# Fit the model on the training dataset
full_fit <- full_workflow %>% fit(data = train_data)

# Print model summary
print(tidy(full_fit$fit$fit))

```

*Interpreation:* - The full model shows that SEX is the only significant predictor of Y (p = 0.046), meaning that drug exposure differs between males and females.
However, DOSE, AGE, WT, and HT are not statistically significant, indicating they do not strongly influence Y.
Since DOSE is still not significant, I may need to explore nonlinear relationships or interaction effects to better explain drug exposure

# Evaluate Model Performance (RMSE)

*Since RMSE (Root Mean Squared Error) is our optimization metric, let’s compute it for both models on the training dataset* Since RMSE (Root Mean Squared Error) is my evaluation metric, I calculated it for both models to compare their accuracy.
The lower the RMSE, the better the model's fit.

```{r}
# Compute mean of Y in training data
y_mean <- mean(train_data$Y)

# Compute RMSE manually for null model
rmse_null <- sqrt(mean((train_data$Y - y_mean)^2))

# Print RMSE for null model
print(paste("RMSE for Null Model:", rmse_null))

```

```{r}
# Define a function to compute RMSE
compute_rmse <- function(model_fit, data) {
  predictions <- predict(model_fit, new_data = data) %>%
    bind_cols(data)  # Merge predictions with actual data
  
  # Compute RMSE
  rmse_value <- predictions %>%
    metrics(truth = Y, estimate = .pred) %>%
    filter(.metric == "rmse") %>%
    pull(.estimate)
  
  return(rmse_value)
}

# Compute RMSE for both models
rmse_simple <- compute_rmse(simple_fit, train_data)
rmse_full <- compute_rmse(full_fit, train_data)

# Print RMSE values
print(paste("RMSE for Simple Model (Y ~ DOSE):", rmse_simple))
print(paste("RMSE for Full Model (Y ~ All Predictors):", rmse_full))

```

*Interpretation:* - The full model has a lower RMSE (1272.96) than the simple model (1322.99), indicating a small improvement in prediction accuracy when adding more predictors.
However, since the improvement is minimal, it suggests that DOSE and the other variables are not strong predictors of Y.
To improve the model further, I may need to explore nonlinear relationships, interactions, or additional predictors.

*Compute RMSE for the Null Model* - Since the null model always predicts the mean of Y for all observations, I computed its RMSE to establish a baseline for comparison.
This helps me see if my actual models are performing better than simply guessing the average value of Y every time.

```{r}
# Define null model (predicts only the mean of Y)
null_mod <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

# Create a workflow (use Y ~ NULL to remove intercept issue)
null_workflow <- workflow() %>%
  add_model(null_mod) %>%
  add_formula(Y ~ NULL)  # No predictors, just predict mean of Y

# Fit the null model
null_fit <- null_workflow %>% fit(data = train_data)

# Compute mean of Y in training data
y_mean <- mean(train_data$Y)

# Compute RMSE manually for null model
rmse_null_manual <- sqrt(mean((train_data$Y - y_mean)^2))

# Print RMSE for null model
print(paste("RMSE for Null Model (Manual Calculation):", rmse_null_manual))


```

*Interpretation:* - The null model RMSE is 1329.39, meaning predicting the mean of Y for all observations results in an average error of 1329.39 units.
If my actual models have similar or higher RMSE, it suggests that DOSE, AGE, SEX, WT, and HT are weak predictors.
This indicates I may need better features, transformations, or a different modeling approach to improve accuracy.

*Compare RMSE Values* - I printed the RMSE values for the null model, simple model (Y \~ DOSE), and full model (Y \~ All Predictors) to compare their performance.
If the null model’s RMSE is close to the others, it suggests that my predictors aren't adding much value, and I may need to explore better features or a different modeling approach.

```{r}
# Print RMSE values
print(paste("RMSE for Null Model:", rmse_null))
print(paste("RMSE for Simple Model (Y ~ DOSE):", rmse_simple))
print(paste("RMSE for Full Model (Y ~ All Predictors):", rmse_full))

```

*Interpretation:* - The full model (RMSE = 1272.96) performed slightly better than the simple model (1322.99) and the null model (1329.39), but the improvement is minimal.
This suggests that DOSE, AGE, SEX, WT, and HT contribute little predictive power, and a more advanced modeling approach may be needed for better accuracy.

*10-Fold Cross-Validation for Model Evaluation* - To get a more reliable estimate of how well my models perform on unseen data, I used 10-fold cross-validation.
This splits the training data into 10 parts, fits the model 10 times (each time leaving out one part for validation), and computes an average RMSE.
This approach helps me avoid overfitting and ensures my model generalizes better beyond just the training set

```{r}
# Set seed for cross-validation
set.seed(1234)

# Perform 10-fold cross-validation on training data
folds <- vfold_cv(train_data, v = 10)

# Fit the simple model with cross-validation
cv_simple_fit <- simple_workflow %>% fit_resamples(folds)

# Fit the full model with cross-validation
cv_full_fit <- full_workflow %>% fit_resamples(folds)

# Compute RMSE for both models using CV
cv_rmse_simple <- collect_metrics(cv_simple_fit) %>%
  filter(.metric == "rmse") %>%
  pull(mean)

cv_rmse_full <- collect_metrics(cv_full_fit) %>%
  filter(.metric == "rmse") %>%
  pull(mean)

# Compute standard error for RMSE (model variability)
cv_se_simple <- collect_metrics(cv_simple_fit) %>%
  filter(.metric == "rmse") %>%
  pull(std_err)

cv_se_full <- collect_metrics(cv_full_fit) %>%
  filter(.metric == "rmse") %>%
  pull(std_err)

# Print cross-validation RMSE and standard error
print(paste("Cross-Validated RMSE for Simple Model:", cv_rmse_simple, "±", cv_se_simple))
print(paste("Cross-Validated RMSE for Full Model:", cv_rmse_full, "±", cv_se_full))

```

*Interpretation:* - The cross-validation RMSE for the simple model (1326.55 ± 113.22) is slightly lower and has more variability than the full model (1350.10 ± 90.57), suggesting that adding more predictors did not improve model performance.
Since both models have high RMSE and overlap within their standard errors, it indicates that DOSE, AGE, SEX, WT, and HT are not strong predictors of Y, and alternative modeling approaches may be needed.

------------------------------------------------------------------------

# This section added by Murtaza Yaqubi

```{r}
library(dplyr)
library(ggplot2)
library(tidymodels)
library(rsample)
library(purrr)
library(tidyr)
```

# Combine predictions from the three models (Null, Simple, and Full)

```{r}
# 1. Predictions from the simple model (only DOSE as predictor)
dose_pred <- predict(simple_fit, new_data = train_data) %>% 
  bind_cols(train_data) %>%
  select(.pred, Y) %>%
  mutate(model = "Dose")

# 2. Predictions from the full model (all predictors: DOSE, AGE, SEX, WT, HT)
all_pred <- predict(full_fit, new_data = train_data) %>% 
  bind_cols(train_data) %>%
  select(.pred, Y) %>%
  mutate(model = "All Predictors")

# 3. Predictions from the null model (predicts the mean of Y)
null_pred <- train_data %>% 
  mutate(.pred = mean(Y)) %>%
  select(.pred, Y) %>%
  mutate(model = "Null")

# Combine all predictions into one dataset
combined_preds <- bind_rows(dose_pred, all_pred, null_pred)
```

# Plot Observed vs. Predicted Values for all models (combined)

```{r}
# Combined plot
ggplot(combined_preds, aes(x = Y, y = .pred, color = model, shape = model)) +
  geom_point(alpha = 0.5, size = 4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  scale_color_manual(values = c("All Predictors" = "purple", 
                                "Dose" = "orange", 
                                "Null" = "red")) +
  scale_shape_manual(values = c("All Predictors" = 18,  
                                "Dose" = 17,       
                                "Null" = 19)) +
  theme_minimal() +
  labs(title = "Observed vs. Predicted Values",
       x = "Observed Values",
       y = "Predicted Values",
       color = "Model",
       shape = "Model")
```

All three models appear on the same panel.
The null model’s horizontal band near 4000 is unchanged, the dose-only model still forms three tight clusters, and the full model’s predictions scatter around the diagonal line.
While the full model appears more flexible, it continues to underpredict some higher observed values.

# Plot Observed vs. Predicted Values for all models (faceted)

```{r}
# Faceted plot (separate panel for each model)
ggplot(combined_preds, aes(x = Y, y = .pred, color = model, shape = model)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  scale_color_manual(values = c("All Predictors" = "blue", 
                                "Dose" = "darkgreen", 
                                "Null" = "firebrick")) +
  scale_shape_manual(values = c("All Predictors" = 18,  
                                "Dose" = 17,       
                                "Null" = 19)) +
  theme_minimal() +
  labs(title = "Observed vs. Predicted Values by Model",
       x = "Observed Values",
       y = "Predicted Values") +
  facet_wrap(~model)
```

Each panel corresponds to a different model.
The null model’s predictions cluster on a single horizontal line near 4000, reflecting a constant mean prediction.
The dose-only model produces three distinct horizontal lines matching its discrete dose levels.
The full model shows points scattered between about 3000 and 4500, closer to the diagonal but still leaving a noticeable gap from perfect alignment.

# Residual Plot for Full Model

```{r}
# Compute residuals for the full model predictions
m2_res <- all_pred %>%
  mutate(residual = .pred - Y)

# Plot predicted values vs residuals
ggplot(m2_res, aes(x = .pred, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "solid", color = "purple") +
  theme_minimal() +
  labs(title = "Predicted vs. Residuals (Full Model)",
       x = "Predicted Values",
       y = "Residuals")
```

In this plot, each point represents the difference between the full model’s predicted value and the observed value (the residual) plotted against the predicted value.
Most predictions fall between 3000 and 5000 on the x-axis, while the residuals range from roughly -3000 to +2000 on the y-axis.
The points are scattered around the horizontal zero line, suggesting that some predictions are above and some below the observed values, but there appears to be a mild pattern in which higher predicted values often have more negative residuals, indicating that the model tends to underpredict when the true outcome is large.
Overall, the distribution of residuals implies that, although the model captures some variation, there is still unexplained structure in the data that may warrant additional predictors or a more flexible modeling approach.

# Bootstrap to Assess Prediction Uncertainty for Full Model

```{r}
# Reset the random seed for bootstrap reproducibility
set.seed(rngseed)

# Create 100 bootstrap samples from train_data, stratifying by DOSE if desired
dat_bs <- bootstraps(train_data, times = 100, strata = DOSE)

# Initialize an empty list to store bootstrap predictions
pred_list <- vector("list", length(dat_bs$splits))

# Loop over each bootstrap sample
for (i in seq_along(dat_bs$splits)) {
  # Extract the bootstrap sample
  dat_sample <- analysis(dat_bs$splits[[i]])
  
  # Fit the full model on the bootstrap sample using the full workflow
  model_fit <- full_workflow %>% fit(data = dat_sample)
  
  # Make predictions on the original training data and store them
  pred_list[[i]] <- predict(model_fit, new_data = train_data) %>% pull(.pred)
}

# Combine the list of predictions into a matrix (rows: bootstrap iterations, columns: observations)
pred_bs <- do.call(cbind, pred_list)

# Compute quantiles (5.5%, 50%, 94.5%) for each observation across bootstrap iterations
boot_preds <- apply(pred_bs, 1, quantile, probs = c(0.055, 0.5, 0.945)) %>% t()

# Convert to a data frame with appropriate column names
preds_df <- data.frame(
  observed = train_data$Y,
  predicted = rowMeans(pred_bs),   # Mean prediction from bootstraps (could be used as point estimate)
  lower = boot_preds[, 1],           # Lower bound (5.5% quantile)
  median = boot_preds[, 2],          # Median prediction (50% quantile)
  upper = boot_preds[, 3]            # Upper bound (94.5% quantile)
)

# Reshape predictions for plotting (comparing median and mean predictions)
preds_long <- preds_df %>%
  pivot_longer(cols = c(median, predicted), names_to = "Type", values_to = "Value")
```

# Plot Observed vs Predicted with Bootstrap Confidence Intervals

```{r}
ggplot() + 
  # Error bars for 89% CI
  geom_errorbar(data = preds_df, 
                aes(x = observed, ymin = lower, ymax = upper, color = "CI"), 
                width = 0, alpha = 0.6, size = 0.8) +
  # Median predictions (red points)
  geom_point(data = filter(preds_long, Type == "median"), 
             aes(x = observed, y = Value, color = "Median"), size = 4) +  
  # Mean predictions (orange diamond)
  geom_point(data = filter(preds_long, Type == "predicted"), 
             aes(x = observed, y = Value, color = "Mean"), size = 3, shape = 18) +  
  # 45° reference line
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  
  labs(title = "Observed vs. Predicted with Bootstrap Confidence Intervals",
       x = "Observed Values",
       y = "Predicted Values",
       color = "Legend") +
  scale_color_manual(values = c("CI" = "darkblue", "Median" = "pink", "Mean" = "purple"),
                     labels = c("89% CI", "Median", "Mean")) +
  theme_minimal()

```

In this plot, each observation’s true value is on the x-axis, while the model’s bootstrap-derived predictions and confidence intervals appear on the y-axis.
The pink circles (median) and purple points (mean) generally cluster between 3000 and 5000, even though observed values range beyond 6000, indicating a tendency to underpredict higher outcomes.
The vertical bars, representing the 89% confidence intervals, widen notably at larger observed values, reflecting increased uncertainty in that range.
The diagonal reference line highlights how closely (or loosely) each prediction aligns with the actual measurement, and many points lie below it at higher observed values, confirming that the model systematically underestimates at the upper end of Y.
